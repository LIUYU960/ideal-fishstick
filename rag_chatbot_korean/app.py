import os
import streamlit as st
from dotenv import load_dotenv
from typing import List, Dict
from pypdf import PdfReader

from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate

load_dotenv()
st.set_page_config(page_title="ë‚˜ë§Œì˜ RAG ì±—ë´‡ (ì´ˆê°„ë‹¨ ì•ˆì •íŒ)", page_icon="ğŸ¤–", layout="wide")

st.title("ğŸ¤– ë‚˜ë§Œì˜ RAG ì±—ë´‡ (ì´ˆê°„ë‹¨ ì•ˆì •íŒ)")
st.caption("íŒŒì¼ ì—…ë¡œë“œ â†’ ì²­í¬ ë¶„í•  â†’ í‚¤ì›Œë“œ ì ìˆ˜ë¡œ ìƒìœ„ ë¬¸ë§¥ ì„ íƒ â†’ LLM ë‹µë³€ (ê·¼ê±° í‘œì‹œ)")

# ì„¸ì…˜ ìƒíƒœ
if "splits" not in st.session_state:
    st.session_state.splits: List[Dict] = []
if "history" not in st.session_state:
    st.session_state.history = []

# ì‚¬ì´ë“œë°”
st.sidebar.header("âš™ï¸ ì„¤ì •")
api_key = os.getenv("OPENAI_API_KEY")
if not api_key:
    st.sidebar.error("Secretsì— OPENAI_API_KEYë¥¼ ë“±ë¡í•˜ì„¸ìš”.")
else:
    st.sidebar.success("OPENAI_API_KEY ê°ì§€ë¨")

# ì—…ë¡œë“œ
uploaded = st.file_uploader("ğŸ“„ PDF/TXT ì—…ë¡œë“œ (ì—¬ëŸ¬ ê°œ ê°€ëŠ¥)", type=["pdf", "txt"], accept_multiple_files=True)

col1, col2 = st.columns(2)
with col1:
    chunk_size = st.number_input("ì²­í¬ í¬ê¸°", 200, 3000, 1000, 100)
with col2:
    chunk_overlap = st.number_input("ì²­í¬ ì¤‘ì²©", 0, 800, 150, 50)

def read_pdf(file) -> str:
    pdf = PdfReader(file)
    texts = []
    for p in pdf.pages:
        texts.append(p.extract_text() or "")
    return "\n".join(texts)

def read_txt(file) -> str:
    return file.read().decode("utf-8", errors="ignore")

def split_text(text: str, size: int, overlap: int) -> List[str]:
    chunks = []
    i = 0
    while i < len(text):
        chunks.append(text[i:i+size])
        i += max(1, size - overlap)
    return chunks

if st.button("ğŸ”¨ ì¸ë±ìŠ¤ êµ¬ì¶•"):
    if not uploaded:
        st.warning("ë¨¼ì € íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.")
    else:
        splits = []
        for f in uploaded:
            name = getattr(f, "name", "uploaded")
            if name.lower().endswith(".pdf"):
                content = read_pdf(f)
            else:
                content = read_txt(f)
            for ch in split_text(content, int(chunk_size), int(chunk_overlap)):
                if ch.strip():
                    splits.append({"text": ch, "source": name})
        st.session_state.splits = splits
        st.success(f"ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ! ì²­í¬ ìˆ˜: {len(splits)}")

# ê°„ë‹¨ í‚¤ì›Œë“œ ì ìˆ˜ ê¸°ë°˜ ê²€ìƒ‰
def rank_by_keywords(query: str, splits: List[Dict], k: int = 4) -> List[Dict]:
    q_tokens = [t for t in query.lower().split() if t.strip()]
    scores = []
    for s in splits:
        text = s["text"].lower()
        score = sum(text.count(t) for t in q_tokens) + 1e-6  # ìµœì†Œê°’
        scores.append((score, s))
    scores.sort(key=lambda x: x[0], reverse=True)
    return [s for _, s in scores[:k]]

st.subheader("ğŸ’¬ ì§ˆë¬¸í•˜ê¸°")
q = st.text_input("ì˜ˆ: ì´ ë¬¸ì„œì˜ í•µì‹¬ ìš”ì•½ì€?")
top_k = st.slider("ê²€ìƒ‰ ë¬¸ì„œ ìˆ˜ (k)", 1, 10, 4)
ask = st.button("ì§ˆë¬¸ ë³´ë‚´ê¸°")

if ask:
    if not st.session_state.splits:
        st.warning("ë¨¼ì € ì¸ë±ìŠ¤ë¥¼ êµ¬ì¶•í•˜ì„¸ìš”.")
    elif not q.strip():
        st.warning("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”.")
    else:
        try:
            top_docs = rank_by_keywords(q, st.session_state.splits, int(top_k))
            context = "\n\n---\n\n".join([d["text"] for d in top_docs])
            sources = ", ".join(sorted({d["source"] for d in top_docs}))

            template = """
ë‹¹ì‹ ì€ ì—…ë¡œë“œëœ ìë£Œë¥¼ ê·¼ê±°ë¡œ ì •í™•í•˜ê³  ê°„ê²°í•˜ê²Œ í•œêµ­ì–´ë¡œ ë‹µë³€í•©ë‹ˆë‹¤.
[ìë£Œ]
{context}

# ---- ì§ˆë¬¸ ----
st.subheader("ğŸ’¬ ì§ˆë¬¸í•˜ê¸°")
q = st.text_input("ì˜ˆ: ì´ ë¬¸ì„œì˜ í•µì‹¬ ìš”ì•½ì€?")
top_k = st.slider("ê²€ìƒ‰ ë¬¸ì„œ ìˆ˜ (k)", 1, 10, 4)
ask = st.button("ì§ˆë¬¸ ë³´ë‚´ê¸°")

def split_text(text: str, size: int, overlap: int):
    chunks, i = [], 0
    step = max(1, size - overlap)
    while i < len(text):
        chunks.append(text[i:i+size])
        i += step
    return chunks

def ensure_index_from_uploads():
    # ì´ë¯¸ êµ¬ì¶•ë˜ì–´ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    if st.session_state.get("splits"):
        return
    if not uploaded:
        return
    tmp = []
    for f in uploaded:
        name = getattr(f, "name", "uploaded")
        if name.lower().endswith(".pdf"):
            from pypdf import PdfReader
            pdf = PdfReader(f)
            text = "\n".join([(p.extract_text() or "") for p in pdf.pages])
        elif name.lower().endswith(".txt"):
            text = f.read().decode("utf-8", errors="ignore")
        else:
            continue  # åªæ”¯æŒ pdf/txt
        for ch in split_text(text, int(chunk_size), int(chunk_overlap)):
            if ch.strip():
                tmp.append({"text": ch, "source": name})
    st.session_state.splits = tmp

if ask:
    # â‘  è‡ªåŠ¨å°è¯•å»ºç´¢å¼•
    if not st.session_state.get("splits"):
        with st.spinner("ì¸ë±ìŠ¤ë¥¼ ìë™ìœ¼ë¡œ êµ¬ì¶•í•˜ëŠ” ì¤‘..."):
            ensure_index_from_uploads()

    # â‘¡ ä»ç„¶æ²¡æœ‰å¯ç”¨æ–‡æœ¬ï¼Œç»™å‡ºæç¤º
    if not st.session_state.get("splits"):
        st.warning("ë¨¼ì € íŒŒì¼(.txt/.pdf)ì„ ì—…ë¡œë“œí•˜ê³  ì¸ë±ìŠ¤ë¥¼ êµ¬ì¶•í•˜ì„¸ìš”.")
    elif not q.strip():
        st.warning("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”.")
    else:
        # ä¸‹é¢æ˜¯ä½ çš„æ£€ç´¢ä¸å›ç­”é€»è¾‘ï¼ˆä¿æŒä¸å˜ï¼‰
        top_docs = rank_by_keywords(q, st.session_state.splits, int(top_k))
        context = "\n\n---\n\n".join([d["text"] for d in top_docs])
        sources = ", ".join(sorted({d["source"] for d in top_docs}))
        template = """
ë‹¹ì‹ ì€ ì—…ë¡œë“œëœ ìë£Œë¥¼ ê·¼ê±°ë¡œ ì •í™•í•˜ê³  ê°„ê²°í•˜ê²Œ í•œêµ­ì–´ë¡œ ë‹µë³€í•©ë‹ˆë‹¤.
[ìë£Œ]
{context}

# ---- ì§ˆë¬¸ ----
st.subheader("ğŸ’¬ ì§ˆë¬¸í•˜ê¸°")
q = st.text_input("ì˜ˆ: ì´ ë¬¸ì„œì˜ í•µì‹¬ ìš”ì•½ì€?")
top_k = st.slider("ê²€ìƒ‰ ë¬¸ì„œ ìˆ˜ (k)", 1, 10, 4)
ask = st.button("ì§ˆë¬¸ ë³´ë‚´ê¸°")

def split_text(text: str, size: int, overlap: int):
    chunks, i = [], 0
    step = max(1, size - overlap)
    while i < len(text):
        chunks.append(text[i:i+size])
        i += step
    return chunks

def ensure_index_from_uploads():
    # ì´ë¯¸ êµ¬ì¶•ë˜ì–´ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    if st.session_state.get("splits"):
        return
    if not uploaded:
        return
    tmp = []
    for f in uploaded:
        name = getattr(f, "name", "uploaded")
        if name.lower().endswith(".pdf"):
            from pypdf import PdfReader
            pdf = PdfReader(f)
            text = "\n".join([(p.extract_text() or "") for p in pdf.pages])
        elif name.lower().endswith(".txt"):
            text = f.read().decode("utf-8", errors="ignore")
        else:
            continue  # åªæ”¯æŒ pdf/txt
        for ch in split_text(text, int(chunk_size), int(chunk_overlap)):
            if ch.strip():
                tmp.append({"text": ch, "source": name})
    st.session_state.splits = tmp

if ask:
    # â‘  è‡ªåŠ¨å°è¯•å»ºç´¢å¼•
    if not st.session_state.get("splits"):
        with st.spinner("ì¸ë±ìŠ¤ë¥¼ ìë™ìœ¼ë¡œ êµ¬ì¶•í•˜ëŠ” ì¤‘..."):
            ensure_index_from_uploads()

    # â‘¡ ä»ç„¶æ²¡æœ‰å¯ç”¨æ–‡æœ¬ï¼Œç»™å‡ºæç¤º
    if not st.session_state.get("splits"):
        st.warning("ë¨¼ì € íŒŒì¼(.txt/.pdf)ì„ ì—…ë¡œë“œí•˜ê³  ì¸ë±ìŠ¤ë¥¼ êµ¬ì¶•í•˜ì„¸ìš”.")
    elif not q.strip():
        st.warning("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”.")
    else:
        # ä¸‹é¢æ˜¯ä½ çš„æ£€ç´¢ä¸å›ç­”é€»è¾‘ï¼ˆä¿æŒä¸å˜ï¼‰
        top_docs = rank_by_keywords(q, st.session_state.splits, int(top_k))
        context = "\n\n---\n\n".join([d["text"] for d in top_docs])
        sources = ", ".join(sorted({d["source"] for d in top_docs}))
        template = """
ë‹¹ì‹ ì€ ì—…ë¡œë“œëœ ìë£Œë¥¼ ê·¼ê±°ë¡œ ì •í™•í•˜ê³  ê°„ê²°í•˜ê²Œ í•œêµ­ì–´ë¡œ ë‹µë³€í•©ë‹ˆë‹¤.
[ìë£Œ]
{context}

[ì§ˆë¬¸]
{question}

ê·œì¹™:
- ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ë‹µë³€
- ë§ˆì§€ë§‰ ì¤„ì— 'ê·¼ê±°:' ë’¤ì— ì¶œì²˜ íŒŒì¼ëª… ë‚˜ì—´

ë‹µë³€:
"""
        from langchain_openai import ChatOpenAI
        from langchain_core.prompts import PromptTemplate
        llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.2)
        prompt = PromptTemplate.from_template(template)
        resp = llm.invoke(prompt.format(context=context, question=q)).content
        if "ê·¼ê±°:" not in resp:
            resp += f"\n\nê·¼ê±°: {sources if sources else 'ì—…ë¡œë“œ ìë£Œ'}"
        st.session_state.history.append(("user", q))
        st.session_state.history.append(("bot", resp, list(sorted({d['source'] for d in top_docs}))))
